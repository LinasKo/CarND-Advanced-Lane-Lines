{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Composition\n",
    "In this notebook I shall compose all the methods that I worked on so far and develop the final solution. A LOT of tinkering and moving around will be done, and some new features might be added, but I'll likely skip long sections for intermediate steps and directly include it in the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T21:35:24.931178Z",
     "start_time": "2018-08-19T21:35:24.924200Z"
    },
    "tags": [
     "#path"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for root folder of the project...\n",
      "Root folder found. Now working in directory 'C:\\Users\\linas\\projects\\CarND-Advanced-Lane-Lines'\n"
     ]
    }
   ],
   "source": [
    "# Set the right working folder to root folder of the project\n",
    "import os\n",
    "\n",
    "print(\"Looking for root folder of the project...\")\n",
    "for folder_depth in range(100): \n",
    "    if os.path.exists(\".git\"):\n",
    "        root_folder = os.getcwd()\n",
    "        print(\"Root folder found. Now working in directory '%s'\" % os.getcwd())\n",
    "        break\n",
    "    else:\n",
    "        print(\"Going up from '%s'\" % os.getcwd())\n",
    "        os.chdir(\"..\")\n",
    "else:\n",
    "    raise Exception(\"Root folder of the project not found. Terminating.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T21:35:24.946110Z",
     "start_time": "2018-08-19T21:35:24.933147Z"
    },
    "tags": [
     "#init",
     "=>path"
    ]
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from moviepy.editor import clips_array\n",
    "from moviepy.editor import ipython_display\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "class Plotter:\n",
    "    def __init__(self, columns, figsize=(20, 40)):\n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        self.columns = columns\n",
    "        self.images = []\n",
    "        self.extra_plots = []\n",
    "        \n",
    "    def add_img(self, img, title):\n",
    "        assert len(img.shape) == 2 or img.shape[2] == 3\n",
    "        \n",
    "        cmap = None\n",
    "        if len(img.shape) == 2:\n",
    "            cmap = \"gray\"\n",
    "\n",
    "        self.images.append((img, title, cmap))\n",
    "        self.extra_plots.append([])\n",
    "        \n",
    "    def add_extra_to_last_img(self, xs, ys, plot_type):\n",
    "        assert plot_type in [\"line\"]\n",
    "        self.extra_plots[-1].append((xs, ys, plot_type))\n",
    "            \n",
    "    def plot(self):\n",
    "        j = 1  # Current column\n",
    "        i = 0\n",
    "        rows = ceil(len(self.images) // self.columns)\n",
    "        for img_index, (img, title, cmap) in enumerate(self.images):\n",
    "            # Draw iamge\n",
    "            ax = plt.subplot((rows / self.columns + 1) * self.columns, self.columns, i * self.columns + j)\n",
    "            ax.set_title(title)\n",
    "            \n",
    "            # Draw Extras\n",
    "            for xs, ys, plot_type in self.extra_plots[img_index]:\n",
    "                if plot_type == \"line\":\n",
    "                    plt.plot(xs, ys, color='yellow')\n",
    "            \n",
    "            # Show\n",
    "            plt.imshow(img, cmap=\"gray\")\n",
    "            \n",
    "            if j % self.columns == 0:\n",
    "                j = 0\n",
    "                i += 1\n",
    "            j += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T21:35:24.981019Z",
     "start_time": "2018-08-19T21:35:24.948122Z"
    },
    "tags": [
     "#prev-work",
     "=>path",
     "=>init"
    ]
   },
   "outputs": [],
   "source": [
    "def apply_precomputed_undistortion(img, mtx_filename, dist_filename):\n",
    "    mtx = np.load(mtx_filename)\n",
    "    dist = np.load(dist_filename)\n",
    "    undist_img = cv2.undistort(img, mtx, dist)\n",
    "    return undist_img\n",
    "\n",
    "\n",
    "def threshold_image(img):\n",
    "    # Saturation-based thresholding\n",
    "    hls_img = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    s_channel = hls_img[:, :, 2]\n",
    "    \n",
    "    s_thresh_value = 150\n",
    "    s_thresh = np.zeros_like(s_channel)\n",
    "    s_thresh[s_channel > s_thresh_value] = 1\n",
    "    \n",
    "    # Edge Detection\n",
    "    red = img[:, :, 0]\n",
    "    sobel_kernel = 25\n",
    "    assert sobel_kernel >= 3 and sobel_kernel % 2 == 1\n",
    "    sobel_x = cv2.Sobel(red, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobel_y = cv2.Sobel(red, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    \n",
    "    sobel_x = np.absolute(sobel_x)\n",
    "    sobel_y = np.absolute(sobel_y)\n",
    "\n",
    "    # Magnitude\n",
    "    magnitude = (sobel_x**2 + sobel_y**2)**.5\n",
    "    mag_norm = np.uint8(255 * magnitude / np.max(magnitude))\n",
    "    mag_thresh_value = 30\n",
    "    mag_thresh = np.zeros_like(mag_norm, dtype=np.uint8)\n",
    "    mag_thresh[mag_norm > mag_thresh_value] = 1\n",
    "    \n",
    "    # Direction\n",
    "    atan = np.arctan2(sobel_y, sobel_x)\n",
    "    atan_thresh_min = 0.8\n",
    "    atan_thresh_max = 1.2\n",
    "    dir_thresh = np.zeros_like(atan, dtype=np.uint8)\n",
    "    dir_thresh[(atan > atan_thresh_min) & (atan <= atan_thresh_max)] = 1\n",
    "    \n",
    "    # Combinations\n",
    "    mag_and_dir = cv2.bitwise_and(mag_thresh, dir_thresh)    \n",
    "    grad_or_color = cv2.bitwise_or(mag_and_dir, s_thresh)\n",
    "    \n",
    "    return grad_or_color\n",
    "\n",
    "\n",
    "def warp_perspective(img, x_offset=200, y_offset=100, new_image_shape=(1000, 1000), flags=cv2.INTER_LINEAR):    \n",
    "    # Detect Lines\n",
    "    # Luckily, the camera resolution is 1280 x 720 in all iamges / videos we're given. This means I can hardode the values.\n",
    "    top_left  = [578,  463]\n",
    "    top_right = [706,  463]\n",
    "    bot_right = [1043, 677]\n",
    "    bot_left  = [267,  677]\n",
    "    src_corners = np.float32([top_left, top_right, bot_right, bot_left])\n",
    "    \n",
    "    line_img = np.zeros(new_image_shape, dtype=np.uint8)\n",
    "    \n",
    "    new_top_left  = [x_offset, y_offset]\n",
    "    new_top_right = [line_img.shape[1] - x_offset, y_offset]\n",
    "    new_bot_right = [line_img.shape[1] - x_offset, line_img.shape[0] - y_offset]\n",
    "    new_bot_left  = [x_offset, line_img.shape[0] - y_offset]\n",
    "    dst_corners = np.float32([new_top_left, new_top_right, new_bot_right, new_bot_left])\n",
    "    \n",
    "    transform_matrix = cv2.getPerspectiveTransform(src_corners, dst_corners)\n",
    "    inv_transform_matrix = cv2.getPerspectiveTransform(dst_corners, src_corners)\n",
    "    warped = cv2.warpPerspective(img, transform_matrix, line_img.shape[::-1], flags=flags)\n",
    "\n",
    "    return warped, transform_matrix, inv_transform_matrix\n",
    "\n",
    "\n",
    "def retrieve_polylines(warped, draw_windows=True):    \n",
    "    \"\"\"\n",
    "    Takes in the warped thresholds to find polylines on the road\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find Curvature\n",
    "    warped_y_midpoint = warped.shape[0] // 2\n",
    "    histogram = np.sum(warped[warped_y_midpoint:, :], axis=0)\n",
    "    out_img = np.dstack((warped, warped, warped))\n",
    "    \n",
    "    hist_x_midpoint = np.int(histogram.shape[0] // 2)\n",
    "    left_x_base = np.argmax(histogram[:hist_x_midpoint])\n",
    "    right_x_base = np.argmax(histogram[hist_x_midpoint:]) + hist_x_midpoint\n",
    "\n",
    "    # Hyperparameters\n",
    "    win_min_pix = 50  # minimum number of pixels found to recenter window\n",
    "    win_num = 9  # the number of sliding windows\n",
    "    win_half_width = 200 // 2\n",
    "    win_height = np.int(warped.shape[0] // win_num)\n",
    "    \n",
    "    nonzero = warped.nonzero()\n",
    "    nonzero_y = np.array(nonzero[0])\n",
    "    nonzero_x = np.array(nonzero[1])\n",
    "    \n",
    "    left_x_current = left_x_base\n",
    "    right_x_current = right_x_base\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "    \n",
    "    for win_index in range(win_num):\n",
    "        win_y_low = warped.shape[0] - (win_index + 1) * win_height\n",
    "        win_y_high = warped.shape[0] - win_index * win_height\n",
    "        win_x_left_low   = left_x_current - win_half_width\n",
    "        win_x_left_high  = left_x_current  + win_half_width\n",
    "        win_x_right_low  = right_x_current - win_half_width\n",
    "        win_x_right_high = right_x_current + win_half_width\n",
    "        \n",
    "        # Draw the windows on the visualization image\n",
    "        if draw_windows:\n",
    "            cv2.rectangle(out_img, (win_x_left_low,  win_y_low), (win_x_left_high,  win_y_high), color=(0, 255, 0), thickness=3)\n",
    "            cv2.rectangle(out_img, (win_x_right_low, win_y_low), (win_x_right_high, win_y_high), color=(0, 255, 0), thickness=3)\n",
    "        \n",
    "        good_left_inds = (\n",
    "            (nonzero_x >= win_x_left_low) &\n",
    "            (nonzero_x < win_x_left_high) &\n",
    "            (nonzero_y >= win_y_low) &\n",
    "            (nonzero_y < win_y_high)\n",
    "        ).nonzero()[0]\n",
    "        good_right_inds = (\n",
    "            (nonzero_x >= win_x_right_low) &\n",
    "            (nonzero_x < win_x_right_high) &\n",
    "            (nonzero_y >= win_y_low) &\n",
    "            (nonzero_y < win_y_high)\n",
    "        ).nonzero()[0]\n",
    "        \n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "        \n",
    "        if len(good_left_inds) > win_min_pix:\n",
    "            left_x_current = np.int(np.mean(nonzero_x[good_left_inds]))\n",
    "        if len(good_right_inds) > win_min_pix:        \n",
    "            right_x_current = np.int(np.mean(nonzero_x[good_right_inds]))\n",
    "\n",
    "    try:\n",
    "        left_lane_inds = np.concatenate(left_lane_inds)\n",
    "        right_lane_inds = np.concatenate(right_lane_inds)\n",
    "    except ValueError as e:\n",
    "        raise e\n",
    "\n",
    "    left_x = nonzero_x[left_lane_inds]\n",
    "    left_y = nonzero_y[left_lane_inds] \n",
    "    right_x = nonzero_x[right_lane_inds]\n",
    "    right_y = nonzero_y[right_lane_inds]\n",
    "    \n",
    "    ## Visualization ##\n",
    "    # Colors in the left and right lane regions\n",
    "    out_img[left_y, left_x] = [255, 0, 0]\n",
    "    out_img[right_y, right_x] = [0, 0, 255]\n",
    "    \n",
    "    # Polyfit\n",
    "    left_fit = np.polyfit(left_y, left_x, 2)\n",
    "    right_fit = np.polyfit(right_y, right_x, 2)\n",
    "\n",
    "    return out_img, left_fit, right_fit\n",
    "\n",
    "\n",
    "def retrieve_polylines_from_older(warped, left_fit, right_fit, draw_windows=True):\n",
    "    \"\"\"\n",
    "    The same polyfit function, but this time it uses the window around previous polynomial lines to search for pixels.\n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    win_half_width = 200 // 2\n",
    "\n",
    "    nonzero = warped.nonzero()\n",
    "    nonzero_y = np.array(nonzero[0])\n",
    "    nonzero_x = np.array(nonzero[1])\n",
    "\n",
    "    left_lane_inds = (\n",
    "        (nonzero_x > (left_fit[0] * (nonzero_y**2) + left_fit[1] * nonzero_y + left_fit[2] - win_half_width)) &\n",
    "        (nonzero_x < (left_fit[0] * (nonzero_y**2) + left_fit[1] * nonzero_y + left_fit[2] + win_half_width))\n",
    "    )\n",
    "    right_lane_inds = (\n",
    "        (nonzero_x > (right_fit[0]*(nonzero_y**2) + right_fit[1] * nonzero_y + right_fit[2] - win_half_width)) &\n",
    "        (nonzero_x < (right_fit[0]*(nonzero_y**2) + right_fit[1] * nonzero_y + right_fit[2] + win_half_width))\n",
    "    )\n",
    "\n",
    "    left_x = nonzero_x[left_lane_inds]\n",
    "    left_y = nonzero_y[left_lane_inds] \n",
    "    right_x = nonzero_x[right_lane_inds]\n",
    "    right_y = nonzero_y[right_lane_inds]\n",
    "\n",
    "    left_fit = np.polyfit(left_y, left_x, 2)\n",
    "    right_fit = np.polyfit(right_y, right_x, 2)\n",
    "    plot_y = np.linspace(0, warped.shape[0] - 1, warped.shape[0])\n",
    "    \n",
    "    out_img = np.dstack((warped, warped, warped)) * 255\n",
    "    win_img = np.zeros_like(out_img)\n",
    "\n",
    "    out_img[nonzero_y[left_lane_inds],  nonzero_x[left_lane_inds]]  = [255, 0, 0]\n",
    "    out_img[nonzero_y[right_lane_inds], nonzero_x[right_lane_inds]] = [0, 0, 255]\n",
    "\n",
    "    left_line_window_1 = np.array([\n",
    "        np.vstack([left_fitx - win_half_width, ploty]).T\n",
    "    ])\n",
    "    left_line_window_2 = np.array([\n",
    "        np.flipud(np.vstack([left_fitx + win_half_width, ploty]).T)\n",
    "    ])\n",
    "    left_line_pts = np.hstack((left_line_window1, left_line_window2))\n",
    "    \n",
    "    right_line_window1 = np.array([\n",
    "        np.vstack([right_fitx-win_half_width, ploty]).T\n",
    "    ])\n",
    "    right_line_window2 = np.array([\n",
    "        np.flipud(np.vstack([right_fitx + win_half_width, ploty]).T)\n",
    "    ])\n",
    "    right_line_pts = np.hstack((right_line_window1, right_line_window2))\n",
    "\n",
    "    cv2.fillPoly(win_img, np.int_([left_line_pts]), (0,255, 0))\n",
    "    cv2.fillPoly(win_img, np.int_([right_line_pts]), (0,255, 0))\n",
    "    result = cv2.addWeighted(out_img, 1, win_img, 0.3, 0)\n",
    "    \n",
    "    return result, left_fit, right_fit\n",
    "\n",
    "def generate_polyline_plots(warped_shape, left_fit, right_fit):\n",
    "    # Generate x and y values for plotting\n",
    "    plot_y = np.linspace(0, warped_shape[0] - 1, warped_shape[0])\n",
    "\n",
    "    left_fit_x = left_fit[0] * plot_y**2 + left_fit[1] * plot_y + left_fit[2]\n",
    "    right_fit_x = right_fit[0] * plot_y**2 + right_fit[1] * plot_y + right_fit[2]\n",
    "\n",
    "    # # Now call:\n",
    "    # plotter.add_img(out_img, \"warped image\")\n",
    "    # plotter.add_extra_to_last_img(left_fit_x,  plot_y, \"line\")\n",
    "    # plotter.add_extra_to_last_img(right_fit_x, plot_y, \"line\")\n",
    "    \n",
    "    return left_fit_x, right_fit_x, plot_y\n",
    "    \n",
    "def fit_polynomial(x, coefficients):\n",
    "    res = 0\n",
    "    for power, coeff in enumerate(coefficients[::-1]):\n",
    "        res += coeff * x**power\n",
    "    return res\n",
    "\n",
    "def get_lane_area(warped_line_img, inv_transformation_matrix, original_img_shape, left_fit, right_fit):\n",
    "    # Area between two polylines\n",
    "    all_pts = np.zeros(warped_line_img.shape[:2], dtype=np.uint8)\n",
    "    for y in range(warped_line_img.shape[0]):\n",
    "        poly_left = int(fit_polynomial(y, left_fit))\n",
    "        poly_right = int(fit_polynomial(y, right_fit))\n",
    "        all_pts[y, poly_left:poly_right] = 1\n",
    "    \n",
    "    polyfilled = warped_line_img.copy()\n",
    "    polyfilled[all_pts == 1] = np.array([0, 255, 0])\n",
    "    \n",
    "    # Dewarp\n",
    "    dewarped = cv2.warpPerspective(polyfilled, inv_transformation_matrix, original_img_shape[1::-1], flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    return dewarped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T21:35:25Z",
     "start_time": "2018-08-19T21:35:24.984011Z"
    },
    "tags": [
     "#heuristics",
     "=>path",
     "=>init",
     "=>prev-work"
    ]
   },
   "outputs": [],
   "source": [
    "def threshold_yellow_lines(hls_img):\n",
    "    \"\"\"\n",
    "    This function attempts to detect only the yellow lines.\n",
    "    \n",
    "    Why try to make a general function for detecting all lane lines, when we can easily distinguish two types - yellow and white?\n",
    "    (actually, there's a third type, when road side is used as a line, with no explicit line marking)\n",
    "    \n",
    "    I have consulted https://driversed.com/driving-information/signs-signals-and-markings/markings-colors-patterns-meaning.aspx,\n",
    "    verifying that only two color types exist.\n",
    "    \"\"\"\n",
    "    # Convert to hls\n",
    "    h_channel = hls_img[:, :, 0]\n",
    "    l_channel = hls_img[:, :, 1]\n",
    "    s_channel = hls_img[:, :, 2]\n",
    "    \n",
    "    thresh = np.zeros_like(h_channel)\n",
    "    thresh[\n",
    "        (h_channel >= 11) & (h_channel <= 30) &\n",
    "        (s_channel >= 50) &\n",
    "        (l_channel >= 150)\n",
    "    ] = 255\n",
    "    \n",
    "    return thresh\n",
    "\n",
    "\n",
    "def threshold_strong_sat_light(hls_img):\n",
    "    h_channel = hls_img[:, :, 0]\n",
    "    l_channel = hls_img[:, :, 1]\n",
    "    s_channel = hls_img[:, :, 2]\n",
    "    \n",
    "    thresh = np.zeros_like(h_channel)\n",
    "    thresh[\n",
    "        (s_channel >= 150) &\n",
    "        (l_channel >= 128)\n",
    "    ] = 255\n",
    "\n",
    "    return thresh\n",
    "\n",
    "\n",
    "def apply_sobel(channel, kernel_size):\n",
    "    assert kernel_size >= 3 and kernel_size % 2 == 1\n",
    "    sobel_x = cv2.Sobel(channel, cv2.CV_64F, 1, 0, ksize=kernel_size)\n",
    "    sobel_y = cv2.Sobel(channel, cv2.CV_64F, 0, 1, ksize=kernel_size)\n",
    "    \n",
    "    sobel_x = np.absolute(sobel_x)\n",
    "    sobel_y = np.absolute(sobel_y)\n",
    "    \n",
    "    return sobel_x, sobel_y\n",
    "\n",
    "\n",
    "def threshold_magnitude(sobel_x, sobel_y):\n",
    "    magnitude = (sobel_x**2 + sobel_y**2)**.5\n",
    "    mag_norm = np.uint8(255 * magnitude / np.max(magnitude))\n",
    "    mag_thresh_value = 30\n",
    "    mag_thresh = np.zeros_like(mag_norm, dtype=np.uint8)\n",
    "    mag_thresh[mag_norm > mag_thresh_value] = 255\n",
    "    \n",
    "    return mag_thresh\n",
    "\n",
    "\n",
    "def threshold_direction(sobel_x, sobel_y):\n",
    "    atan = np.arctan2(sobel_y, sobel_x)  # returns results in range [0.0, 1.57079632679]\n",
    "    atan_thresh_max = 1.2\n",
    "    dir_thresh = np.zeros_like(atan, dtype=np.uint8)\n",
    "    dir_thresh[(atan <= atan_thresh_max)] = 255\n",
    "    \n",
    "    return dir_thresh\n",
    "\n",
    "\n",
    "def morph_close(thresh, kernel_size):\n",
    "    assert kernel_size >= 3 and kernel_size % 2 == 1\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))\n",
    "    closed = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    return closed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T21:35:25.011935Z",
     "start_time": "2018-08-19T21:35:25.001963Z"
    },
    "tags": [
     "#process",
     "=>prev-work",
     "=>heuristics"
    ]
   },
   "outputs": [],
   "source": [
    "# def process_img(img):\n",
    "    \n",
    "#     # Undistort Image\n",
    "#     mtx_filename  = os.path.join(root_folder, \"params\", \"camera_matrix.npy\")\n",
    "#     dist_filename = os.path.join(root_folder, \"params\", \"dist_coeffs.npy\")\n",
    "#     undistorted = apply_precomputed_undistortion(img, mtx_filename, dist_filename)\n",
    "\n",
    "#     # Threshold image\n",
    "#     thresh = threshold_image(undistorted)\n",
    "\n",
    "#     # Focus on the road lines by warping perspective\n",
    "#     warped, transformation_matrix, inv_transformation_matrix = warp_perspective(thresh, x_offset=200, y_offset=100, new_image_shape=(1000, 1000))\n",
    "\n",
    "#     # Fit a polyline\n",
    "#     warped_line_img, left_fit, right_fit = retrieve_polylines(warped, draw_windows=False)\n",
    "\n",
    "#     # Draw the area\n",
    "#     dewarped = get_lane_area(warped_line_img, inv_transformation_matrix, img.shape, left_fit, right_fit)\n",
    "    \n",
    "#     # Overlay\n",
    "#     alpha = 0.2\n",
    "#     combined_img = np.zeros_like(img)\n",
    "#     cv2.addWeighted(dewarped, alpha, img, 1 - alpha, 0, combined_img)   \n",
    "    \n",
    "#     return combined_img\n",
    "\n",
    "def process_img(rgb_img):\n",
    "    # Undistort Image\n",
    "    mtx_filename  = os.path.join(root_folder, \"params\", \"camera_matrix.npy\")\n",
    "    dist_filename = os.path.join(root_folder, \"params\", \"dist_coeffs.npy\")\n",
    "    undistorted = apply_precomputed_undistortion(rgb_img, mtx_filename, dist_filename)\n",
    "    \n",
    "    # RGB -> HLS\n",
    "    hls_img = cv2.cvtColor(undistorted, cv2.COLOR_RGB2HLS)\n",
    "    h_channel = hls_img[:, :, 0]\n",
    "    l_channel = hls_img[:, :, 1]\n",
    "    s_channel = hls_img[:, :, 2]\n",
    "    \n",
    "    # Find x and y edges\n",
    "    l_sobel_x, l_sobel_y = apply_sobel(l_channel, 25)\n",
    "    s_sobel_x, s_sobel_y = apply_sobel(s_channel, 25)\n",
    "    \n",
    "    # Threshold by color\n",
    "    thresh_yellow = threshold_yellow_lines(hls_img)\n",
    "    thresh_sat_light = threshold_strong_sat_light(hls_img)\n",
    "\n",
    "    # Threshold by edge magnitude\n",
    "    l_thresh_mag = threshold_magnitude(l_sobel_x, l_sobel_y)\n",
    "    s_thresh_mag = threshold_magnitude(s_sobel_x, s_sobel_y)\n",
    "    thresh_mag = cv2.bitwise_or(l_thresh_mag, s_thresh_mag)\n",
    "    \n",
    "    # Threshold by edge direction\n",
    "    l_thresh_dir = threshold_direction(l_sobel_x, l_sobel_y)  # I results are slightly better with just lightness-based threshold than both.\n",
    "    \n",
    "    # Combine edge magnitude and direction\n",
    "    thresh_edges = cv2.bitwise_and(thresh_mag, l_thresh_dir)\n",
    "    \n",
    "    # Fill in gaps between edges\n",
    "    closed_edges = morph_close(thresh_edges, 5)  \n",
    "    \n",
    "    result = closed_edges\n",
    "    \n",
    "\n",
    "    \n",
    "#     # Focus on the road lines by warping perspective\n",
    "#     warped, transformation_matrix, inv_transformation_matrix = warp_perspective(\n",
    "#         undistorted, x_offset=200, y_offset=100, new_image_shape=img.shape[:2]\n",
    "#     )\n",
    "    \n",
    "#     # Examine other color spaces\n",
    "#     hls_warped = cv2.cvtColor(warped, cv2.COLOR_RGB2HLS)\n",
    "#     one_channel = hls_warped[:, :, 1]\n",
    "#     one_channel = np.uint8(one_channel / np.max(one_channel) * 255)\n",
    "    \n",
    "#     clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "#     result = clahe.apply(one_channel)\n",
    "    \n",
    "    \n",
    "#     plt.hist(reduced_hist)\n",
    "#     plt.show()\n",
    "    \n",
    "#     one_channel = np.uint8(one_channel / np.max(one_channel) * 255)\n",
    "    \n",
    "#     s_thresh_value = 150\n",
    "#     s_thresh = np.zeros_like(one_channel)\n",
    "#     s_thresh[one_channel > s_thresh_value] = 1\n",
    "    \n",
    "#     s_thresh3 = cv2.cvtColor(s_thresh * 255, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    #     # Threshold image\n",
    "#     thresh = threshold_image(undistorted)\n",
    "        \n",
    "    if len(np.shape(result)) == 2:\n",
    "        return cv2.cvtColor(result, cv2.COLOR_GRAY2BGR)\n",
    "    else:\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T21:49:06.426906Z",
     "start_time": "2018-08-19T21:35:25.014929Z"
    },
    "tags": [
     "#run-on-videos",
     "=>prev-work",
     "=>process",
     "=>path"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video C:\\Users\\linas\\projects\\CarND-Advanced-Lane-Lines\\results\\challenge_video.mp4\n",
      "[MoviePy] Writing video C:\\Users\\linas\\projects\\CarND-Advanced-Lane-Lines\\results\\challenge_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 485/485 [02:15<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: C:\\Users\\linas\\projects\\CarND-Advanced-Lane-Lines\\results\\challenge_video.mp4 \n",
      "\n",
      "Wall time: 2min 15s\n",
      "[MoviePy] >>>> Building video C:\\Users\\linas\\projects\\CarND-Advanced-Lane-Lines\\results\\harder_challenge_video.mp4\n",
      "[MoviePy] Writing video C:\\Users\\linas\\projects\\CarND-Advanced-Lane-Lines\\results\\harder_challenge_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1199/1200 [05:35<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: C:\\Users\\linas\\projects\\CarND-Advanced-Lane-Lines\\results\\harder_challenge_video.mp4 \n",
      "\n",
      "Wall time: 5min 36s\n",
      "[MoviePy] >>>> Building video C:\\Users\\linas\\projects\\CarND-Advanced-Lane-Lines\\results\\project_video.mp4\n",
      "[MoviePy] Writing video C:\\Users\\linas\\projects\\CarND-Advanced-Lane-Lines\\results\\project_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1260/1261 [05:47<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: C:\\Users\\linas\\projects\\CarND-Advanced-Lane-Lines\\results\\project_video.mp4 \n",
      "\n",
      "Wall time: 5min 47s\n"
     ]
    }
   ],
   "source": [
    "video_in_folder  = os.path.join(root_folder, \"data\")\n",
    "video_out_folder = os.path.join(root_folder, \"results\")\n",
    "video_in_pattern = os.path.join(video_in_folder, \"*.mp4\")\n",
    "video_in_fnames  = glob.glob(video_in_pattern)\n",
    "video_out_fnames = [os.path.join(video_out_folder, os.path.basename(fname)) for fname in video_in_fnames]\n",
    "\n",
    "only_use_videos = []  # Only use videos with indices specified here\n",
    "use_subclip = False  # For testing\n",
    "\n",
    "for i in range(len(video_in_fnames)):\n",
    "    if only_use_videos and i not in only_use_videos:\n",
    "        continue\n",
    "    video_in_fname  = video_in_fnames[i]\n",
    "    video_out_fname = video_out_fnames[i]\n",
    "    \n",
    "    clip = VideoFileClip(video_in_fname)\n",
    "    if use_subclip:\n",
    "        clip = clip.subclip(0, 10)\n",
    "    \n",
    "    processed_clip = clip.fl_image(process_img)\n",
    "\n",
    "    %time processed_clip.write_videofile(video_out_fname, audio=False)\n",
    "    \n",
    "    clip.close()\n",
    "    processed_clip.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T13:32:33.804082Z",
     "start_time": "2018-08-19T13:32:33.349631Z"
    },
    "tags": [
     "=>path",
     "=>init",
     "#combination"
    ]
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "MoviePy error: the file C:\\Users\\linas\\projects\\CarND-Advanced-Lane-Lines\\results\\challenge_video.mp4 could not be found!\nPlease check that you entered the correct path.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-e40e549092c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0moriginal_clip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVideoFileClip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_in_fnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvideo_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mmodified_clip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVideoFileClip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_out_fnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvideo_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0muse_subclip\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0moriginal_clip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moriginal_clip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\moviepy\\video\\io\\VideoFileClip.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, has_mask, audio, audio_buffersize, target_resolution, resize_algorithm, audio_fps, audio_nbytes, verbose, fps_source)\u001b[0m\n\u001b[0;32m     89\u001b[0m                                          \u001b[0mtarget_resolution\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_resolution\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m                                          \u001b[0mresize_algo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresize_algorithm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m                                          fps_source=fps_source)\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;31m# Make some of the reader's attributes accessible from the clip\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\moviepy\\video\\io\\ffmpeg_reader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, print_infos, bufsize, pix_fmt, check_duration, target_resolution, resize_algo, fps_source)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         infos = ffmpeg_parse_infos(filename, print_infos, check_duration,\n\u001b[1;32m---> 33\u001b[1;33m                                    fps_source)\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'video_fps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'video_size'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\moviepy\\video\\io\\ffmpeg_reader.py\u001b[0m in \u001b[0;36mffmpeg_parse_infos\u001b[1;34m(filename, print_infos, check_duration, fps_source)\u001b[0m\n\u001b[0;32m    270\u001b[0m         raise IOError((\"MoviePy error: the file %s could not be found!\\n\"\n\u001b[0;32m    271\u001b[0m                       \u001b[1;34m\"Please check that you entered the correct \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m                       \"path.\")%filename)\n\u001b[0m\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: MoviePy error: the file C:\\Users\\linas\\projects\\CarND-Advanced-Lane-Lines\\results\\challenge_video.mp4 could not be found!\nPlease check that you entered the correct path."
     ]
    }
   ],
   "source": [
    "# Display the videos\n",
    "video_in_folder  = os.path.join(root_folder, \"data\")\n",
    "video_out_folder = os.path.join(root_folder, \"results\")\n",
    "video_in_pattern = os.path.join(video_in_folder, \"*.mp4\")\n",
    "video_in_fnames  = glob.glob(video_in_pattern)\n",
    "video_out_fnames = [os.path.join(video_out_folder, os.path.basename(fname)) for fname in video_in_fnames]\n",
    "\n",
    "video_index = 0\n",
    "use_subclip = False\n",
    "\n",
    "composed_out_folder = os.path.join(root_folder, \"results\", \"intermediate\")\n",
    "composed_out_fname = os.path.join(composed_out_folder, \"yellow_detection_\" + os.path.basename(video_out_fnames[video_index]))\n",
    "\n",
    "original_clip = VideoFileClip(video_in_fnames[video_index]).resize(0.5)\n",
    "modified_clip = VideoFileClip(video_out_fnames[video_index]).resize(0.5)\n",
    "if use_subclip:\n",
    "    original_clip = original_clip.subclip(0, 10)\n",
    "    modified_clip = modified_clip.subclip(0, 10)\n",
    "composed_clip = clips_array([[original_clip, modified_clip]])\n",
    "\n",
    "composed_clip.write_videofile(composed_out_fname, audio=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T09:44:53.591586Z",
     "start_time": "2018-08-19T09:44:53.575631Z"
    }
   },
   "outputs": [],
   "source": [
    "original_clip.close()\n",
    "modified_clip.close()\n",
    "composed_clip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary Tools\n",
    "This should probably be split into a separate notebook, but I need some small tools to tinker with. E.g. I need to save one frame of an image to analyze the hue and lightness value of yellow line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T15:02:49.613927Z",
     "start_time": "2018-08-19T15:02:25.805976Z"
    },
    "tags": [
     "#aux-tools",
     "=>prev-work",
     "=>heuristics",
     "=>path"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Frames produced: 16\n"
     ]
    }
   ],
   "source": [
    "# video_in_name  = os.path.join(root_folder, \"results\", \"project_video.mp4\")\n",
    "# video_in_name  = os.path.join(root_folder, \"results\", \"harder_challenge_video.mp4\")\n",
    "video_in_name  = os.path.join(root_folder, \"results\", \"intermediate\", \"channels\", \"s challenge_video.mp4\")\n",
    "out_folder = os.path.join(root_folder, \"results\", \"intermediate\", \"dissected\")\n",
    "\n",
    "clip = VideoFileClip(video_in_name)\n",
    "\n",
    "for i in range(int(clip.duration)):\n",
    "    out_filename = os.path.join(out_folder, \"frame_%d.png\" % i)\n",
    "    clip.save_frame(out_filename, t=i)\n",
    "    \n",
    "print(\"Done! Frames produced: %d\" % clip.duration)\n",
    "clip.close()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
